Design of a DeepFake Detection Model for Audio Using Deep Learning Techniques

Abstract:
This study presents a comparative analysis between Mel-Frequency Cepstral Coefficients (MFCC) and Mel Spectrogram features for detecting deepfake voices using deep learning techniques. Through experimentation and evaluation, we investigate the efficacy of each feature set in distinguishing between genuine and synthetic speech. Results indicate varying performance levels, shedding light on the effectiveness of these features in tackling the challenges posed by deepfake voice manipulation. The dataset that was used in this study is from ASVSpoof 2019 dataset. Results show that Long Short-Term Memory (LSTM) outperformed all other deep learning models that were used for MFCC features, achieving 81% accuracy in unforeseen data. Mel Spectrogram using VGG16, on the other hand, achieved 85% accuracy in unforeseen data.

Authors:
Abo, Emmanuel
Alferos, Joshua

Model Deployment Link: https://drive.google.com/drive/folders/1V8UX78yFsprWEbN9c4G4leF8ttn5kNtm?usp=sharing

Paper Link: https://docs.google.com/document/d/1UncBzHyRBXqsytQo73BBh_d9EqGSR1CLpqj6lLIjzlQ/edit?usp=sharing
